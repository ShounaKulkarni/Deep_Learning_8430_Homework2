{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6376393-99c4-4e48-af03-965c1ca23de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python sequence.py /scratch1/shounak/MLDS_hw2_data/training_data/feat/ /scratch1/shounak/Manish/MLDS_hw2_data/training_label.json \n",
    "import sys\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle as pk\n",
    "\n",
    "\n",
    "max_decoder_steps = 20\n",
    "\n",
    "def build_bld_dict(sentences, n_min):\n",
    "    n_word = {}\n",
    "    n_seq = 0\n",
    "    for sentence in sentences:\n",
    "        n_seq += 1\n",
    "        for word in sentence.lower().split(' '):\n",
    "            n_word[word] = n_word.get(word, 0) + 1\n",
    "    \n",
    "    bld_dict = [word for word in n_word if n_word[word] >= n_min]\n",
    "    print ('From %d words filtered %d words to dictionary with minimum count [%d]' % (len(n_word), len(bld_dict), n_min) ,'\\n')\n",
    "\n",
    "    keywordtrans = {}\n",
    "    wordkeytrans = {}\n",
    "    keywordtrans[0] = '<pad>'\n",
    "    keywordtrans[1] = '<bos>'\n",
    "    keywordtrans[2] = '<eos>'\n",
    "    keywordtrans[3] = '<unk>'    \n",
    "    wordkeytrans['<pad>'] = 0\n",
    "    wordkeytrans['<bos>'] = 1\n",
    "    wordkeytrans['<eos>'] = 2\n",
    "    wordkeytrans['<unk>'] = 3\n",
    "\n",
    "    for key, word in enumerate(bld_dict):\n",
    "        wordkeytrans[word] = key + 4\n",
    "        keywordtrans[key + 4] = word\n",
    "\n",
    "    n_word['<pad>'] = n_seq\n",
    "    n_word['<bos>'] = n_seq\n",
    "    n_word['<eos>'] = n_seq\n",
    "    n_word['<unk>'] = n_seq\n",
    "    \n",
    "    return wordkeytrans, keywordtrans, bld_dict\n",
    "\n",
    "def pad_seqs(seqs, max_len=None, pad_str='pre', trunc_str='pre', value=0):    \n",
    "  \n",
    "    len_seq = list()\n",
    "    for se in seqs:\n",
    "        len_seq.append(len(se))\n",
    "\n",
    "    n_sample = len(seqs)\n",
    "    if max_len is None:\n",
    "        max_len = np.max(len_seq)\n",
    "\n",
    "    seq_shape = tuple()\n",
    "    for shp in seqs:\n",
    "        if len(shp) > 0:\n",
    "            seq_shape = np.asarray(shp).shape[1:]\n",
    "            break\n",
    "\n",
    "    padseq = (np.zeros((n_sample, max_len) + seq_shape) * value).astype('int32')\n",
    "    for k, se in enumerate(seqs):\n",
    "        if not len(se):\n",
    "            continue  # empty list/array was found\n",
    "        if trunc_str == 'pre':\n",
    "            trunc = se[-max_len:]\n",
    "        elif trunc_str == 'post':\n",
    "            trunc = se[:max_len]\n",
    "\n",
    "        trunc = np.asarray(trunc, dtype='int32')\n",
    "        if trunc.shape[1:] != seq_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s ''is different from expected shape %s' %(trunc.shape[1:], k, seq_shape))\n",
    "\n",
    "        if pad_str == 'post':\n",
    "            padseq[k, :len(trunc)] = trunc\n",
    "        elif pad_str == 'pre':\n",
    "            padseq[k, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % pad_str)\n",
    "    return padseq\n",
    "    \n",
    "def filter_token(string):\n",
    "    filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    for c in filters:\n",
    "        string = string.replace(c,'')\n",
    "    return string\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(2022)\n",
    "\n",
    "    feat_folder = sys.argv[1]\n",
    "    training_label_json = sys.argv[2]\n",
    "\n",
    "    feat_filenames = listdir(feat_folder)\n",
    "    feat_filepaths = [(feat_folder + filename) for filename in feat_filenames]\n",
    "\n",
    "    \n",
    "    vid_id = [filename[:-4] for filename in feat_filenames]# Remove '.avi' from filename\n",
    "\n",
    "    dict_feat = {}\n",
    "    for filepath in feat_filepaths:\n",
    "        video_feat = np.load(filepath)\n",
    "        video_ID = filepath[: -4].replace(feat_folder, \"\")\n",
    "        dict_feat[video_ID] = video_feat\n",
    "    \n",
    "    video_caption = json.load(open(training_label_json, 'r'))\n",
    "    dict_caption={}\n",
    "    captions_corpus = list()\n",
    "    for video in video_caption:\n",
    "        filtered_captions = [filter_token(sentence) for sentence in video[\"caption\"]]\n",
    "        dict_caption[video[\"id\"]] = filtered_captions\n",
    "        captions_corpus += filtered_captions\n",
    "\n",
    "\n",
    "    wordkeytrans, keywordtrans, bld_dict = build_bld_dict(captions_corpus, n_min=3)\n",
    "    \n",
    "    pk.dump(wordkeytrans, open('./wordkeytrans.obj', 'wb'))\n",
    "    pk.dump(keywordtrans, open('./keywordtrans.obj', 'wb'))\n",
    "\n",
    "    ID_caption = list()\n",
    "    captions_words = list()\n",
    "\n",
    "    words_list = list()\n",
    "    for ID in vid_id:\n",
    "        for caption in dict_caption[ID]:\n",
    "            ID_caption.append((dict_feat[ID], caption))\n",
    "            words = caption.split()\n",
    "            captions_words.append(words)\n",
    "            for word in words:\n",
    "                words_list.append(word)\n",
    "\n",
    "    caption_set = np.unique(words_list, return_counts=True)[0]\n",
    "    max_captions_length = max([len(words) for words in captions_words])\n",
    "    avg_captions_length = np.mean([len(words) for words in captions_words])\n",
    "    num_unique_tokens_captions = len(caption_set)\n",
    "\n",
    "    print(\"Caption dimenstions are:\", np.shape(ID_caption))\n",
    "    print(\"Caption's max length is:\", max_captions_length)\n",
    "    print(\"Average length of captions is:\", avg_captions_length)\n",
    "    print(\"Unique tokens are:\", num_unique_tokens_captions)\n",
    "    \n",
    "    print(\"ID of 17th video:\", vid_id[16])\n",
    "    print(\"Shape of features of 17th video:\", ID_caption[16][0].shape)\n",
    "    print(\"Caption of 17th video:\", ID_caption[16][1])\n",
    "   \n",
    "    pk.dump(vid_id, open('vid_id.obj', 'wb'))\n",
    "    pk.dump(dict_caption, open('dict_caption.obj', 'wb'))\n",
    "    pk.dump(dict_feat, open('dict_feat.obj', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
