{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e4a2c-6424-4a69-a24a-832ec790f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python train.py /scratch1/shounak/MLDS_hw2_data/testing_data/feat/ ./scratch1/shounak/MLDS_hw2_data/testing_label.json ./output_testset.txt \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import pickle as pk\n",
    "import warnings\n",
    "\n",
    "from bleu_eval import BLEU\n",
    "from sequence import pad_seqs as ps\n",
    "from seq2seq_model import Seq2Seq_Model\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(2000)\n",
    "    rd.seed(2000)\n",
    "    \n",
    "    \n",
    "    test_feat_folder = sys.argv[1]\n",
    "    testing_label_json = sys.argv[2]\n",
    "    output_testset = sys.argv[3]\n",
    "\n",
    "    tf.app.flags.DEFINE_integer('nnet_size', 1024, 'Number of hidden units per layer')\n",
    "    tf.app.flags.DEFINE_integer('n_layer', 2, 'Number of layers per encoder and decoder')\n",
    "    tf.app.flags.DEFINE_integer('feature_dim', 4096, 'Feature dimensions per video frame')\n",
    "    tf.app.flags.DEFINE_float('lambda_r', 0.001, 'Learning rate')\n",
    "    tf.app.flags.DEFINE_integer('batch_size', 50, 'Batch size')\n",
    "    tf.app.flags.DEFINE_integer('num_epochs', 200, 'number of epochs')\n",
    "    \n",
    "    tf.app.flags.DEFINE_integer('embedding_size', 1024, 'Embedding dimensions of encoder and decoder inputs')\n",
    "\n",
    "    tf.app.flags.DEFINE_float('max_grad_norm', 5.0, 'Maximum gradient norm')\n",
    "    \n",
    "    tf.app.flags.DEFINE_integer('sample_size', 1450, 'train data sample')\n",
    "    tf.app.flags.DEFINE_integer('frame_dim', 80, '# of frame per video')\n",
    "\n",
    "    tf.app.flags.DEFINE_boolean('use_attention', True, 'Attention Enabled')  \n",
    "    \n",
    "    tf.app.flags.DEFINE_boolean('beam_search', False, 'Beam search Disabled')\n",
    "    tf.app.flags.DEFINE_integer('beam_size', 5, 'Size of beam search')\n",
    "    \n",
    "    tf.app.flags.DEFINE_integer('max_encoder_steps', 64, 'Maximum encoder steps')\n",
    "    tf.app.flags.DEFINE_integer('max_decoder_steps', 15, 'Maximum decoder steps')\n",
    "    \n",
    "    tf.app.flags.DEFINE_string('model_dir', 'models/', 'model directory')\n",
    "    tf.app.flags.DEFINE_string('model_name', 's2s.ckpt', 'Checkpoints file name')\n",
    "\n",
    "    FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "    num_top_BLEU = 10 #number considered\n",
    "    \n",
    "    top_BLEU = list()\n",
    "\n",
    "    print ('Printing pickle generated file:')\n",
    "    \n",
    "    wordkeytrans = pk.load(open('wordkeytrans.obj', 'rb'))\n",
    "    keywordtrans = pk.load(open('keywordtrans.obj', 'rb'))\n",
    "    video_IDs = pk.load(open('vid_id.obj', 'rb'))\n",
    "    dict_caption = pk.load(open('dict_caption.obj', 'rb'))\n",
    "    dict_feat = pk.load(open('dict_feat.obj', 'rb'))\n",
    "    keywordtrans_srs = pd.Series(keywordtrans)\n",
    "\n",
    "    \n",
    "    test_feat_file = os.listdir(test_feat_folder) \n",
    "    test_feat_path = [(test_feat_folder + filename) for filename in test_feat_file] \n",
    "    test_video_IDs = [filename[:-4] for filename in test_feat_file] \n",
    "\n",
    "    test_dict_feat = {}\n",
    "    \n",
    "    for path in test_feat_path:\n",
    "        test_video_feat = np.load(path) \n",
    "        \n",
    "        sampled_video_frame = sorted(rd.sample(range(FLAGS.frame_dim), FLAGS.max_encoder_steps))\n",
    "        test_video_feat = test_video_feat[sampled_video_frame]\n",
    "\n",
    "        test_video_ID = path[: -4].replace(test_feat_folder, \"\")\n",
    "        test_dict_feat[test_video_ID] = test_video_feat\n",
    "    \n",
    "\n",
    "    test_vid_caption = json.load(open(testing_label_json, 'r'))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        model = Seq2Seq_Model(nnet_size=FLAGS.nnet_size, n_layer=FLAGS.n_layer, feature_dim=FLAGS.feature_dim, embedding_size=FLAGS.embedding_size, \n",
    "            lambda_r=FLAGS.lambda_r, wordkeytrans=wordkeytrans, mode='train', max_grad_norm=FLAGS.max_grad_norm, use_attention=FLAGS.use_attention, \n",
    "            beam_search=FLAGS.beam_search, beam_size=FLAGS.beam_size, max_encoder_steps=FLAGS.max_encoder_steps, max_decoder_steps=FLAGS.max_decoder_steps)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(FLAGS.model_dir)\n",
    "        \n",
    "        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "            print('Model Reload is done ')\n",
    "            model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print('Building a new model')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        summary_file = tf.summary.FileWriter(FLAGS.model_dir, graph=sess.graph)\n",
    "\n",
    "        for epoch in range(FLAGS.num_epochs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            sampled_ID_caption = list()\n",
    "            for ID in video_IDs:\n",
    "                sampled_caption = rd.sample(dict_caption[ID], 1)[0]\n",
    "                sampled_video_frame = sorted(rd.sample(range(FLAGS.frame_dim), FLAGS.max_encoder_steps))\n",
    "                sampled_video_feat = dict_feat[ID][sampled_video_frame]\n",
    "                sampled_ID_caption.append((sampled_video_feat, sampled_caption))\n",
    "\n",
    "            \n",
    "            rd.shuffle(sampled_ID_caption)\n",
    "\n",
    "            for batch_start, batch_end in zip(range(0, FLAGS.sample_size, FLAGS.batch_size), range(FLAGS.batch_size, FLAGS.sample_size, FLAGS.batch_size)):\n",
    "                print (\"Training done for batch:%04d/%04d\" %(batch_end, FLAGS.sample_size))\n",
    "\n",
    "                batch_sampled_ID_caption = sampled_ID_caption[batch_start : batch_end]\n",
    "                batch_video_feats = [elements[0] for elements in batch_sampled_ID_caption]\n",
    "                batch_video_frame = [FLAGS.max_decoder_steps] * FLAGS.batch_size\n",
    "                batch_captions = np.array([\"<bos> \"+ elements[1] for elements in batch_sampled_ID_caption])\n",
    "\n",
    "                for index, caption in enumerate(batch_captions):\n",
    "                    caption_words = caption.lower().split(\" \")\n",
    "                    if len(caption_words) < FLAGS.max_decoder_steps:\n",
    "                        batch_captions[index] = batch_captions[index] + \" <eos>\"\n",
    "                    else:\n",
    "                        new_caption = \"\"\n",
    "                        for i in range(FLAGS.max_decoder_steps - 1):\n",
    "                            new_caption = new_caption + caption_words[i] + \" \"\n",
    "                        batch_captions[index] = new_caption + \"<eos>\"\n",
    "\n",
    "                batch_captions_words_index = list()\n",
    "                for caption in batch_captions:\n",
    "                    words_index = list()\n",
    "                    for caption_words in caption.lower().split(' '):\n",
    "                        if caption_words in wordkeytrans:\n",
    "                            words_index.append(wordkeytrans[caption_words])\n",
    "                        else:\n",
    "                            words_index.append(wordkeytrans['<unk>'])\n",
    "                    batch_captions_words_index.append(words_index)\n",
    "\n",
    "                batch_captions_matrix = ps(batch_captions_words_index, pad_str='post', max_len=FLAGS.max_decoder_steps)\n",
    "                batch_captions_length = [len(x) for x in batch_captions_matrix]\n",
    "               \n",
    "                loss, summary = model.train(sess, batch_video_feats, batch_video_frame, batch_captions_matrix, batch_captions_length)            \n",
    "               \n",
    "            #Validation on test data set#\n",
    "\n",
    "            test_caption_list = list()\n",
    "            for batch_start, batch_end in zip(range(0, len(test_video_IDs) + FLAGS.batch_size, FLAGS.batch_size), range(FLAGS.batch_size, len(test_video_IDs) + FLAGS.batch_size, FLAGS.batch_size)):\n",
    "                print (\"%04d/%04d\" %(batch_end, FLAGS.sample_size))\n",
    "                if batch_end < len(test_video_IDs):\n",
    "                    batch_sampled_ID = np.array(test_video_IDs[batch_start : batch_end])\n",
    "                    batch_video_feats = [test_dict_feat[x] for x in batch_sampled_ID]\n",
    "                else:\n",
    "                    batch_sampled_ID = test_video_IDs[batch_start : batch_end]\n",
    "                    for _ in range(batch_end - len(test_video_IDs)):\n",
    "                        batch_sampled_ID.append(test_video_IDs[-1])\n",
    "                    batch_sampled_ID = np.array(batch_sampled_ID)\n",
    "                    batch_video_feats = [test_dict_feat[x] for x in batch_sampled_ID]\n",
    "\n",
    "                batch_video_frame = [FLAGS.max_decoder_steps] * FLAGS.batch_size \n",
    "\n",
    "                batch_caption_words_index, logits = model.infer(\n",
    "                    sess, \n",
    "                    batch_video_feats, \n",
    "                    batch_video_frame) \n",
    "\n",
    "                if batch_end < len(test_video_IDs):\n",
    "                    batch_caption_words_index = batch_caption_words_index\n",
    "                else:\n",
    "                    batch_caption_words_index = batch_caption_words_index[:len(test_video_IDs) - batch_start]\n",
    "\n",
    "                for index, test_caption_words_index in enumerate(batch_caption_words_index):\n",
    "                    \n",
    "\n",
    "                    if FLAGS.beam_search:\n",
    "                        logits = np.array(logits).reshape(-1, FLAGS.beam_size)\n",
    "                        max_logits_index = np.argmax(np.sum(logits, axis=0))\n",
    "                        predict_list = np.ndarray.tolist(test_caption_words_index[0, :, max_logits_index])\n",
    "                        predict_seq = [keywordtrans[idx] for idx in predict_list]\n",
    "                        test_caption_words = predict_seq\n",
    "                    else:\n",
    "                        test_caption_words_index = np.array(test_caption_words_index).reshape(-1)\n",
    "                        test_caption_words = keywordtrans_srs[test_caption_words_index]\n",
    "                        test_caption = ' '.join(test_caption_words) \n",
    "\n",
    "                    test_caption = ' '.join(test_caption_words)\n",
    "                    test_caption = test_caption.replace('<bos> ', '')\n",
    "                    test_caption = test_caption.replace('<eos>', '')\n",
    "                    test_caption = test_caption.replace(' <eos>', '')\n",
    "                    test_caption = test_caption.replace('<pad> ', '')\n",
    "                    test_caption = test_caption.replace(' <pad>', '')\n",
    "                    test_caption = test_caption.replace(' <unk>', '')\n",
    "                    test_caption = test_caption.replace('<unk> ', '')\n",
    "\n",
    "                    if (test_caption == \"\"):\n",
    "                        test_caption = '.'\n",
    "\n",
    "                   \n",
    "                    test_caption_list.append(test_caption)\n",
    "             \n",
    "                    \n",
    "            df = pd.DataFrame(np.array([test_video_IDs, test_caption_list]).T)\n",
    "            df.to_csv(output_testset, index=False, header=False)\n",
    "            \n",
    "            \n",
    "            result = {}\n",
    "            with open(output_testset, 'r') as test_file:\n",
    "                for line in test_file:\n",
    "                    line = line.rstrip()\n",
    "                    test_id, caption = line.split(',')\n",
    "                    result[test_id] = caption\n",
    "                    \n",
    "            bleu= list()\n",
    "            for item in test_vid_caption:\n",
    "                score_per_video = list()\n",
    "                captions = [x.rstrip('.') for x in item['caption']]\n",
    "                score_per_video.append(BLEU(result[item['id']],captions,True))\n",
    "                bleu.append(score_per_video[0])\n",
    "            avg = sum(bleu) / len(bleu)\n",
    "            print(\"The Average BLEU Score of model is: \" + str(avg))\n",
    "\n",
    "            if (len(top_BLEU) < num_top_BLEU):\n",
    "                top_BLEU.append(avg)\n",
    "                print (\"Saving model with BLEU Score : %.4f ...\" %(avg))\n",
    "                model.saver.save(sess, './models/model' + str(avg)[2:6], global_step=epoch)\n",
    "            else:\n",
    "                if (avg > min(top_BLEU)):\n",
    "                    # Remove min. BLEU score.\n",
    "                    top_BLEU.remove(min(top_BLEU))\n",
    "                    top_BLEU.append(avg)\n",
    "                    print (\"Saving model with BLEU score: %.4f\" %(avg))\n",
    "                    model.saver.save(sess, './models/model' + str(avg)[2:6], global_step=epoch)\n",
    "                    \n",
    "                    \n",
    "            top_BLEU.sort(reverse=True)\n",
    "            \n",
    "            print (\"Highest [%d] BLEU scores: \" %(num_top_BLEU), [\"%.4f\" % x for x in top_BLEU])\n",
    "\n",
    "            print (\"Epoch# %d, Loss: %.4f, Average BLEU score: %.4f, Time taken: %.2fs\" %(epoch, loss, avg, (time.time() - start_time)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
